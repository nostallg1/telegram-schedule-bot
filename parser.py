import requests
from bs4 import BeautifulSoup
import logging
import re
import time
import random
import os
import html

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

BASE_URL = "https://student.lpnu.ua"
SCRAPER_API_KEY = os.environ.get('SCRAPER_API_KEY', None)

# --- Config ---
DAY_MAP = {
    "–ü–æ–Ω–µ–¥—ñ–ª–æ–∫": ["–ø–Ω", "–ø–æ–Ω", "mon"],
    "–í—ñ–≤—Ç–æ—Ä–æ–∫":  ["–≤—Ç", "–≤—ñ–≤", "bt", "vt", "tue"],
    "–°–µ—Ä–µ–¥–∞":    ["—Å—Ä", "—Å–µ—Ä", "cp", "wed"],
    "–ß–µ—Ç–≤–µ—Ä":    ["—á—Ç", "—á–µ—Ç", "thu"],
    "–ü'—è—Ç–Ω–∏—Ü—è":  ["–ø—Ç", "–ø—è—Ç", "fri"],
    "–°—É–±–æ—Ç–∞":    ["—Å–±", "—Å—É–±", "sat"],
    "–ù–µ–¥—ñ–ª—è":    ["–Ω–¥", "–Ω–µ–¥", "sun"]
}

def get_standard_day_name(line):
    clean_line = re.sub(r'[^\w]', '', line).lower()
    for standard_name, variants in DAY_MAP.items():
        for variant in variants:
            if clean_line.startswith(variant):
                return standard_name
    return None

def escape_markdown(text):
    return text

def make_request(group_name, semester, duration):
    schedule_url = f"{BASE_URL}/students_schedule"
    # –§–æ—Ä–º—É—î–º–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏
    params = {
        "studygroup_abbrname": group_name,
        "semestr": semester,
        "semestrduration": duration
    }
    
    if SCRAPER_API_KEY:
        # –°–ø—Ä–æ—â—É—î–º–æ –∑–∞–ø–∏—Ç –¥–æ ScraperAPI
        # –í–∏–º–∏–∫–∞—î–º–æ keep_headers, —â–æ–± ScraperAPI —Å–∞–º –ø—ñ–¥—ñ–±—Ä–∞–≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏
        payload = {
            'api_key': SCRAPER_API_KEY,
            'url': schedule_url + '?' + requests.compat.urlencode(params),
            'render': 'true' # JS rendering –∑–∞–ª–∏—à–∞—î–º–æ, –≤—ñ–Ω –∫–æ—Ä–∏—Å–Ω–∏–π
        }
        logger.info(f"ScraperAPI URL: {payload['url']}") # –õ–æ–≥—É—î–º–æ URL –¥–ª—è –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏
        response = requests.get('http://api.scraperapi.com', params=payload, timeout=60)
    else:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': BASE_URL + '/',
        }
        time.sleep(1 + random.random())
        with requests.Session() as session:
            session.headers.update(headers)
            response = session.get(schedule_url, params=params, timeout=15)
            
    return response

def fetch_schedule_dict(group_name, semester="1", duration="1", subgroup=None, week_filter=None):
    
    # –ö–†–û–ö 1: –ó–∞–ø–∏—Ç
    try:
        response = make_request(group_name, semester, "1")
        if response.status_code != 200:
             return {"Info": f"‚ùå –ü–æ–º–∏–ª–∫–∞ HTTP {response.status_code}."}
    except Exception as e:
        logger.error(f"Network error: {e}")
        return {"Info": "‚ùå –ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è."}

    soup = BeautifulSoup(response.text, 'html.parser')
    content_div = soup.find('div', class_='view-content')

    # --- –î–Ü–ê–ì–ù–û–°–¢–ò–ö–ê (–©–æ –±–∞—á–∏—Ç—å –±–æ—Ç?) ---
    # –Ø–∫—â–æ –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –Ω–µ–º–∞—î –¥–Ω—ñ–≤, —Å–ø—Ä–æ–±—É—î–º–æ –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —á–æ–º—É
    days = []
    if content_div:
        days = content_div.find_all('div', class_='view-grouping')
    
    if not days:
        # –Ø–∫—â–æ —Ü–µ –Ω–µ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏–π —Ä–æ–∑–∫–ª–∞–¥, —Ü–µ –º–æ–∂–µ –±—É—Ç–∏ –ø–æ–º–∏–ª–∫–∞ —Å–∞–π—Ç—É
        if not content_div:
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —á–∏ —Ü–µ –Ω–µ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –∑–∞—Ö–∏—Å—Ç—É
            page_text = soup.get_text(separator=" ", strip=True)
            if "security" in page_text.lower() or "challenge" in page_text.lower():
                return {"Info": "üõ° –ë–æ—Ç –Ω–∞—Ç—Ä–∞–ø–∏–≤ –Ω–∞ –∑–∞—Ö–∏—Å—Ç (Cloudflare). –°–ø—Ä–æ–±—É–π—Ç–µ —â–µ —Ä–∞–∑ —á–µ—Ä–µ–∑ —Ö–≤–∏–ª–∏–Ω—É."}
            if "–Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ" in page_text.lower():
                return {"Info": f"‚ùå –°–∞–π—Ç –∫–∞–∂–µ: –ì—Ä—É–ø—É <b>{html.escape(group_name)}</b> –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ."}
            
            # –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —à–º–∞—Ç–æ–∫ —Ç–µ–∫—Å—Ç—É –¥–ª—è –Ω–∞–ª–∞–≥–æ–¥–∂–µ–Ω–Ω—è
            return {"Info": f"‚ö†Ô∏è –î–∏–≤–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å —Å–∞–π—Ç—É (–Ω–µ–º–∞—î view-content). –ü–æ—á–∞—Ç–æ–∫ —Ç–µ–∫—Å—Ç—É:\n{html.escape(page_text[:200])}"}
        
        # –Ø–∫—â–æ content_div —î, –∞–ª–µ –¥–Ω—ñ–≤ –Ω–µ–º–∞—î
        raw_text = content_div.get_text(separator="\n", strip=True)
        if len(raw_text) < 20:
             return {"Info": f"üì≠ –°–∞–π—Ç –ø–æ–≤–µ—Ä–Ω—É–≤ –ø–æ—Ä–æ–∂–Ω—é —Ç–∞–±–ª–∏—Ü—é –¥–ª—è <b>{group_name}</b>."}
        
        # –Ø–∫—â–æ —Ç–µ–∫—Å—Ç —î, –∞–ª–µ –º–∏ –π–æ–≥–æ –Ω–µ —Ä–æ–∑–ø—ñ–∑–Ω–∞–ª–∏ - –ø–æ–∫–∞–∂–µ–º–æ –π–æ–≥–æ!
        return {"Info": f"‚ö†Ô∏è –ù–µ –º–æ–∂—É —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ —Ñ–æ—Ä–º–∞—Ç. –û—Å—å —â–æ –±–∞—á—É:\n\n{html.escape(raw_text[:500])}"}

    # --- –Ø–ö–©–û –í–°–ï –û–ö, –ü–ê–†–°–ò–ú–û ---
    schedule_data = {} 

    # ... (–∫–æ–¥ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—ó is_pair_for_excluded_subgroup - —Ç–∞–∫–∏–π —Å–∞–º–∏–π, —è–∫ –±—É–≤) ...
    def is_pair_for_excluded_subgroup(text, current_subgroup):
        if not current_subgroup: return False
        excluded_subgroup = str(3 - int(current_subgroup))
        patterns = [f"\({excluded_subgroup}\)", f"–ø—ñ–¥–≥—Ä\.\s*{excluded_subgroup}", f"{excluded_subgroup}\s*–ø/–≥"]
        text_lower = text.lower()
        for p in patterns:
            if re.search(p, text_lower, re.IGNORECASE):
                our_sub = str(current_subgroup)
                if not re.search(f"\({our_sub}\)", text_lower): return True 
        return False

    try:
        for day_block in days:
            header = day_block.find('span', class_='view-grouping-header')
            raw_day = header.get_text(strip=True) if header else ""
            day_name = get_standard_day_name(raw_day)
            if not day_name: continue 
            
            day_text = f"üìÖ <b>{day_name}</b> ({html.escape(group_name)})\n\n"
            has_pairs = False
            
            rows = day_block.find_all('div', class_='stud_schedule')
            for row in rows:
                classes = row.get('class', [])
                if week_filter == 'chys' and 'znam' in classes: continue
                if week_filter == 'znam' and 'chys' in classes: continue

                num_header = row.find_previous('h3')
                pair_num = num_header.get_text(strip=True) if num_header else "?"
                content = row.find('div', class_='group_content')
                if not content: content = row
                full_pair_text = content.get_text(separator=" ", strip=True).strip()

                if is_pair_for_excluded_subgroup(full_pair_text, subgroup): continue
                
                safe_text = html.escape(full_pair_text)
                week_mark = " <i>(—á–∏—Å.)</i>" if 'chys' in classes else (" <i>(–∑–Ω–∞–º.)</i>" if 'znam' in classes else "")

                day_text += f"‚è∞ <b>{pair_num} –ø–∞—Ä–∞</b>{week_mark}\nüìñ {safe_text}\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
                has_pairs = True
            
            if has_pairs:
                schedule_data[day_name] = day_text

        if not schedule_data:
            return {"Info": "üì≠ –†–æ–∑–∫–ª–∞–¥ –ø–æ—Ä–æ–∂–Ω—ñ–π (–º–æ–∂–ª–∏–≤–æ, —Ñ—ñ–ª—å—Ç—Ä–∏ –ø—Ä–∏—Ö–æ–≤–∞–ª–∏ –≤—Å—ñ –ø–∞—Ä–∏)."}

        return schedule_data

    except Exception as e:
        logger.error(f"Parser Logic Error: {e}", exc_info=True)
        return {"Info": f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –∫–æ–¥—É: {e}"}
        


