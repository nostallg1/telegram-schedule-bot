import requests
from bs4 import BeautifulSoup
import logging
import re
import time
import random
import os
import html

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

BASE_URL = "https://student.lpnu.ua"
SCRAPER_API_KEY = os.environ.get('SCRAPER_API_KEY', None)

# --- CONFIG ---
DAY_MAP = {
    "–ü–æ–Ω–µ–¥—ñ–ª–æ–∫": ["–ø–Ω", "–ø–æ–Ω", "mon"],
    "–í—ñ–≤—Ç–æ—Ä–æ–∫":  ["–≤—Ç", "–≤—ñ–≤", "bt", "vt", "tue"],
    "–°–µ—Ä–µ–¥–∞":    ["—Å—Ä", "—Å–µ—Ä", "cp", "wed"],
    "–ß–µ—Ç–≤–µ—Ä":    ["—á—Ç", "—á–µ—Ç", "thu"],
    "–ü'—è—Ç–Ω–∏—Ü—è":  ["–ø—Ç", "–ø—è—Ç", "fri"],
    "–°—É–±–æ—Ç–∞":    ["—Å–±", "—Å—É–±", "sat"],
    "–ù–µ–¥—ñ–ª—è":    ["–Ω–¥", "–Ω–µ–¥", "sun"]
}

def get_standard_day_name(line):
    clean_line = re.sub(r'[^\w]', '', line).lower()
    for standard_name, variants in DAY_MAP.items():
        for variant in variants:
            if clean_line.startswith(variant):
                return standard_name
    return None

def escape_markdown(text):
    return text

# --- –í–ù–£–¢–†–Ü–®–ù–Ø –§–£–ù–ö–¶–Ü–Ø –ó–ê–ü–ò–¢–£ ---
def make_request(group_name, semester, duration):
    """–†–æ–±–∏—Ç—å –æ–¥–∏–Ω –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∏–π –∑–∞–ø–∏—Ç –¥–æ —Å–∞–π—Ç—É"""
    schedule_url = f"{BASE_URL}/students_schedule"
    params = {
        "studygroup_abbrname": group_name,
        "semestr": semester,
        "semestrduration": duration
    }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
        'Referer': BASE_URL + '/',
    }
    
    # –ó–∞—Ç—Ä–∏–º–∫–∞ (—Ç—ñ–ª—å–∫–∏ –¥–ª—è –ø—Ä—è–º–∏—Ö –∑–∞–ø–∏—Ç—ñ–≤, –¥–ª—è ScraperAPI –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ)
    if not SCRAPER_API_KEY:
        time.sleep(0.5 + random.random())

    if SCRAPER_API_KEY:
        payload = {
            'api_key': SCRAPER_API_KEY,
            'url': schedule_url + '?' + requests.compat.urlencode(params),
            'render': 'true' # JS rendering –¥–æ–ø–æ–º–∞–≥–∞—î
        }
        response = requests.get('http://api.scraperapi.com', params=payload, timeout=60)
    else:
        with requests.Session() as session:
            session.headers.update(headers)
            response = session.get(schedule_url, params=params, timeout=15)
            
    return response

# --- –ì–û–õ–û–í–ù–ê –§–£–ù–ö–¶–Ü–Ø ---
def fetch_schedule_dict(group_name, semester="1", duration="1", subgroup=None, week_filter=None):
    
    # –ö–†–û–ö 1: –ü—Ä–æ–±—É—î–º–æ –∑–∞–ø–∏—Ç –∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º (Duration=1)
    logger.info(f"Trying {group_name} with duration=1...")
    try:
        response = make_request(group_name, semester, "1") # –°–ø–æ—á–∞—Ç–∫—É —à—É–∫–∞—î–º–æ –≤ "–ü–µ—Ä—à—ñ–π –ø–æ–ª–æ–≤–∏–Ω—ñ"
        response.raise_for_status()
    except Exception as e:
        logger.error(f"Network error: {e}")
        return {"Info": "‚ùå –ü–æ–º–∏–ª–∫–∞ –∑'—î–¥–Ω–∞–Ω–Ω—è."}

    soup = BeautifulSoup(response.text, 'html.parser')
    content_div = soup.find('div', class_='view-content')

    # –ö–†–û–ö 2: –Ø–∫—â–æ –ø—É—Å—Ç–æ, –ø—Ä–æ–±—É—î–º–æ Duration=2 (–î—Ä—É–≥–∞ –ø–æ–ª–æ–≤–∏–Ω–∞ —Å–µ–º–µ—Å—Ç—Ä—É)
    # –¶–µ "–ø–ª–∞–Ω –ë", —è–∫—â–æ –≤ –ø–µ—Ä—à—ñ–π –ø–æ–ª–æ–≤–∏–Ω—ñ –Ω—ñ—á–æ–≥–æ –Ω–µ–º–∞—î
    if not content_div or not content_div.find_all('div', class_='view-grouping'):
        logger.info(f"Empty/Not found for duration=1. Trying duration=2...")
        try:
            response_2 = make_request(group_name, semester, "2")
            if response_2.status_code == 200:
                soup_2 = BeautifulSoup(response_2.text, 'html.parser')
                content_div_2 = soup_2.find('div', class_='view-content')
                if content_div_2 and content_div_2.find_all('div', class_='view-grouping'):
                    # –£—Ä–∞! –ó–Ω–∞–π—à–ª–∏ —Ä–æ–∑–∫–ª–∞–¥ —É –¥—Ä—É–≥—ñ–π –ø–æ–ª–æ–≤–∏–Ω—ñ
                    soup = soup_2
                    content_div = content_div_2
        except:
            pass # –Ø–∫—â–æ —ñ —Ç—É—Ç –ø–æ–º–∏–ª–∫–∞, –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä—à–æ–≥–æ –∑–∞–ø–∏—Ç—É

    # --- –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É ---
    if not content_div:
        if "–Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ" in soup.text.lower():
            return {"Info": f"‚ùå –ì—Ä—É–ø—É <b>{html.escape(group_name)}</b> –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ."}
        return {"Info": "‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –¥–∞–Ω—ñ."}

    schedule_data = {} 

    # --- Subgroup Filter ---
    def is_pair_for_excluded_subgroup(text, current_subgroup):
        if not current_subgroup: return False
        excluded_subgroup = str(3 - int(current_subgroup))
        # –ü–∞—Ç–µ—Ä–Ω–∏ –¥–ª—è –≤–∏–∫–ª—é—á–µ–Ω–Ω—è
        patterns = [f"\({excluded_subgroup}\)", f"–ø—ñ–¥–≥—Ä\.\s*{excluded_subgroup}", f"{excluded_subgroup}\s*–ø/–≥"]
        text_lower = text.lower()
        
        for p in patterns:
            if re.search(p, text_lower, re.IGNORECASE):
                # –Ø–∫—â–æ —î –º–∞—Ä–∫–µ—Ä –≤–∏–∫–ª—é—á–µ–Ω–æ—ó –≥—Ä—É–ø–∏, –ø–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ –Ω–µ–º–∞—î –º–∞—Ä–∫–µ—Ä–∞ –Ω–∞—à–æ—ó
                our_sub = str(current_subgroup)
                if not re.search(f"\({our_sub}\)", text_lower): 
                    return True 
        return False

    # --- PARSING ---
    try:
        days = content_div.find_all('div', class_='view-grouping')
        if days:
            for day_block in days:
                header = day_block.find('span', class_='view-grouping-header')
                raw_day = header.get_text(strip=True) if header else ""
                day_name = get_standard_day_name(raw_day)
                if not day_name: continue 
                
                day_text = f"üìÖ <b>{day_name}</b> ({html.escape(group_name)})\n\n"
                has_pairs = False
                
                rows = day_block.find_all('div', class_='stud_schedule')
                for row in rows:
                    # Week Filter
                    classes = row.get('class', [])
                    if week_filter == 'chys' and 'znam' in classes: continue
                    if week_filter == 'znam' and 'chys' in classes: continue

                    num_header = row.find_previous('h3')
                    pair_num = num_header.get_text(strip=True) if num_header else "?"
                    
                    content = row.find('div', class_='group_content')
                    if not content: content = row
                    full_pair_text = content.get_text(separator=" ", strip=True).strip()

                    if is_pair_for_excluded_subgroup(full_pair_text, subgroup):
                        continue
                    
                    safe_text = html.escape(full_pair_text)
                    week_mark = ""
                    if 'chys' in classes: week_mark = " <i>(—á–∏—Å.)</i>"
                    if 'znam' in classes: week_mark = " <i>(–∑–Ω–∞–º.)</i>"

                    day_text += f"‚è∞ <b>{pair_num} –ø–∞—Ä–∞</b>{week_mark}\nüìñ {safe_text}\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
                    has_pairs = True
                
                if has_pairs:
                    schedule_data[day_name] = day_text

        if not schedule_data:
            return {"Info": "üì≠ –†–æ–∑–∫–ª–∞–¥ –ø–æ—Ä–æ–∂–Ω—ñ–π –¥–ª—è –æ–±—Ä–∞–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤."}

        return schedule_data

    except Exception as e:
        logger.error(f"Parser Logic Error: {e}", exc_info=True)
        return {"Info": "‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –æ–±—Ä–æ–±–∫–∏."}

